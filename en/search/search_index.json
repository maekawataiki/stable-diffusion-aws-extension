{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>The Extension for Stable Diffusion on AWS solution helps customers migrate their existing Stable Diffusion model training, inference, and finetuning workloads from on-premises servers to Amazon SageMaker using extension and CloudFormation template. By leveraging elastic resources in the cloud, it accelerates model iteration and avoids performance bottlenecks associated with single-server deployments.</p> <p>This implementation guide provides an overview of the solution, its reference architecture and components, considerations for planning the deployment, configuration steps for deploying the solution to the Amazon Web Services (AWS) Cloud. </p>"},{"location":"concepts/","title":"Concepts","text":"<p>This section describes key concepts and defines terminology specific to this solution: </p>"},{"location":"cost/","title":"Cost","text":"<p>You are responsible for the cost of AWS services used when running this solution. </p>"},{"location":"cost/#use-auto-scale-amazon-sagemaker-inference-endpoint-for-image-inference","title":"Use auto-scale Amazon SageMaker Inference Endpoint for Image Inference","text":"<p>As of December 2023, for example, assuming that use will actively inference image for 8 hours per day, 20 working days per month(Assuming that using the standard Stable Diffusion XL model to generate a 1024 X 1024 image takes an average of 7 seconds. This running time can generate 72000 images in one month), the estimated cost of using this solution in the US East (Virginia)(us-east-1) is $310.45 per month.</p> Service Usage Cost/Month Amazon SageMaker $276.00 Number of models per endpoint (1), Storage (General Purpose SSD (gp2)), Instance name (ml.g5.2xlarge), Number of models deployed (1), Number of instances per endpoint (1), Endpoint hour(s) per day (8), Endpoint day(s) per month (20), Storage amount (240 GB per month) AWS Lambda $0.02 Architecture (x86), Architecture (x86), Invoke Mode (Buffered), Amount of ephemeral storage allocated (10 GB), Number of requests (300000 per month), Concurrency (10) Amazon API Gateway $1.05 REST API request units (thousands), Cache memory size (GB) (None), WebSocket message units (thousands), HTTP API requests units (thousands), Average message size (32 KB), Requests ( per month), Requests (300 per month) Amazon Simple Storage Service (S3) $27.70 S3 Standard storage (1000 GB per month), PUT, COPY, POST, LIST requests to S3 Standard (), GET, SELECT, and all other requests from S3 Standard (1000000), Data returned by S3 Select (1000 GB per month) DT Inbound: All other regions (0 TB per month), DT Outbound: Internet (40 GB per month) Amazon DynamoDB $0.50 Table class (Standard), Average item size (all attributes) (1 KB), Data storage size (2 GB) AWS Step Functions $0.13 Workflow requests (600 per month), State transitions per workflow (15) Amazon CloudWatch $5.05 Standard Logs: Data Ingested (10 GB) Total $310.45"},{"location":"cost/#use-amazon-sagemaker-to-train-model","title":"Use Amazon SageMaker to Train Model","text":"<p>Assume that on the basis of inference images, users train for 300 hours per month as the calculation standard (using Kohya, fine-tuning a new safetensor model based on Stable Diffusion V1.5, 1000 steps of iteration, takes 387 seconds. 300 hours of training means one month of training 2790 models), the estimated cost of using this solution would increase by $526.18 in US East (N. Virginia) (us-east-1).</p> Service Usage Cost/Month Amazon SageMaker $526.18 Storage (General Purpose SSD (gp2)), Instance name (ml.g5.2xlarge), Number of training jobs per month (300), Number of instances per job (1), Hour(s) per instance per job (1)"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":"<p>Q: What is Extension for Stable Diffusion on AWS?</p> <p>Extension for Stable Diffusion on AWS is an AWS solution that aims to assists customers migrate their Stable Diffusion model training, inference, and finetuning workloads on Stable Diffusion WebUI from local servers to Amazon SageMaker by providing extension and AWS CloudFormation template. By leveraging elastic cloud resources, this solution accelerates model iteration and mitigates performance bottlenecks associated with single-server deployments. </p> <p>Q: What are the native features/third-party extensions of Stable Diffusion WebUI supported by this solution?</p> <p>This solution supports multiply native features/third-party extensions of Stable Diffusion WebUI. More details can be found at Features and Benefits.</p> <p>Q: What is the licence of this solution?</p> <p>This solution is provided under the Apache-2.0 license. It is a permissive free software license written by the Apache Software Foundation. It allows users to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software under the terms of the license, without concern for royalties.</p> <p>Q: How can I submit a feature request or bug report?</p> <p>You can submit feature requests and bug report through the GitHub issues. Here are the templates for feature request, bug report.</p>"},{"location":"faq/#installation-and-configuration","title":"Installation and Configuration","text":"<p>Q: Is there a specific order for installing third-party plugins and the plugins for this solution?</p> <p>Currently, it is recommended that users install the third-party extensions supported by this solution before installing the solution's own extension. However, the installation order can be changed as well. In that case, a restart of the WebUI is required to ensure the successful functioning of the features.</p> <p>Q: After successfully installing the web UI, I am unable to access it in my browser. How can I resolve this?</p> <p>Before attempting to access the webUI link in your browser, please ensure that the necessary ports are open and not being blocked by a firewall.</p> <p>Q: How can I update the solution?</p> <p>Currently, it is recommended that users avoid frequently updating solutions by deploying stacks through CloudFormation. If updates are necessary, it is advised to uninstall the existing solution stack and then deploy a new stack based on the CloudFormation template. For all the future deployments using CloudFormation, in the 'Bucket' field, enter the S3 bucket name used in the previous deployment, and choose 'yes' for 'DeployedBefore' to ensure a successful redeployment of CloudFormation.</p> <p>Q: How can I change the login user on the same computer?</p> <p>You can switch to another user account by opening a new incognito browser window and logging in with the alternate user credentials.</p> <p>Q: How can I remove the local inference option so that my webUI can only support cloud inference?</p> <p>You can open the WebUI, navigate to tab Settings and select section User interface on the left session bar. Find field\u2018[info] Quick settings list (setting entries that appear at the top of page rather than in settings tab) (requires Reload UI)\u2018, and uncheck \u2018sd_model_checkpoint'. After then, click \u2018Apply setting', and reload webUI from terminal to make the change effective. After reloading the webUI, you will find that the checkpoint selecting drop down list on the upper left side disappeared, and user will only have a cloud inference option. </p>"},{"location":"faq/#pricing","title":"Pricing","text":"<p>Q: How will I be charged and billed for the use of this solution?</p> <p>The solution is free to use, and you are responsible for the cost of AWS services used while running this solution. You pay only for what you use, and there are no minimum or setup fees. Refer to the Centralized Logging with OpenSearch Cost section for detailed cost estimation.</p>"},{"location":"notices/","title":"Notices","text":"<p>Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents Amazon Web Services current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from Amazon Web Services and its affiliates, suppliers or licensors. Amazon Web Services products or services are provided \u201cas is\u201d without warranties, representations, or conditions of any kind, whether express or implied. Amazon Web Services responsibilities and liabilities to its customers are controlled by Amazon Web Services agreements, and this document is not part of, nor does it modify, any agreement between Amazon Web Services and its customers.</p> <p>The Extension for Stable Diffusion on AWS solution is licensed under the terms of the Apache License Version 2.0 available at The Apache Software Foundation</p> <p>This guidance is for informational purposes only.  You should still perform your own independent assessment, and take measures to ensure that you comply with your own specific quality control practices and standards, and the local rules, laws, regulations, licenses and terms of use that apply to you, your content, and the third-party generative AI service referenced in this guidance.  Amazon Web Services has no control or authority over the third-party generative AI service referenced in this guidance, and does not make any representations or warranties that the third-party generative AI service is secure, virus-free, operational, or compatible with your production environment and standards. Amazon Web Services does not make any representations, warranties or guarantees that any information in this guidance will result in a particular outcome or result.   </p>"},{"location":"revisions/","title":"Revisions","text":"Date Version Changes CloudFormation Template 2024/06/28 V1.6.1 - Support automatic template rendering in the frontend when a published template is selected. This facilitates subsequent parameter adjustments and inference based on the selected template.   - Bugs fix V1.6.1 2024/06/14 V1.6.0 - Support ComfyUI on SageMaker  - Support ComfyUI template workflow to release and inference   - Support Amazon SageMaker G6 series inference endpoint(s)  - Support for saving intermediate status of Kohya_ss trained model  - Support for visual evaluation after training Kohya_ss model  - Support image caption through WD14  - Bugs fix V1.6.0 2024/03/22 V1.5.0 - Support webUI V1.8.0  - Support extension ReActor for Stable Diffusion  - Add API Debugger feature  - Support LoRa model training through Kohya_ss  - Improve customer experience on Resource management in webUI  - Optimize the method of customizing BYOC extensions, and support optimization startup  - Support auto-scale ability for real-time inference endpoints  - Support the customizable range for the numbers of instances during auto-scaling  - Optimize installation time by 67%, down to 4 minutes  - Upgrade Python version from 3.9 to 3.10  - Deprecate support for Dreambooth  - Bugs fix V1.5.0 2024/01/19 V1.4.0 - Support webUI V1.7.0  - Support multiple Amazon stacks deployment in one account  - Enhance cold-start time of Amazon SageMaker inference time by 70%, down to 3 mins  - Support image generation through Amazon SageMaker real-time inference endpoint  - Improve customer experience on webUI, containing multiple level filters for tables of roles, users, models and inference endpoints, along with resource delete feature  - Bugs fix V1.4.0 20232/12/13 V1.3.0 - Support Stable Diffusion Turbo inference  - Support Stable Diffusion LCMinference  - Support Stable Diffusion inference through diffuser (including webUI &amp; API direct call) - Support customorize  extensions' combination for SD webUI through BYOC (Bring Your Own Container) - Support user addition and modifications operations to take effect without restarting the webUI  - Bugs fix V1.3.0 2023/10/26 V1.2.1 - Enhance the existing features, for example: add S3 bucket validation, address the issue with Lambda quota.  - Support API direct call support for native Extras  - Bugs fix V1.2.1 2023/9/28 V1.2.0 - Support webUI V1.6.0  - Support controlNet v1.1.410   - Support Stable Diffusion XL 1.0 + ControlNet   - Support Refiner  - Support Multiple User Management  - Support upload model from url to cloud   - Support more efficient Version Update V1.2.0 2023/8/14 V1.1.0 - Support webUI V1.5.1  - Support Stable Diffusion XL 1.0  - Support VAE  - Support upload model from local to S3   - Support Multi-controlnet  - Support deployauto scale SageMaker Endpoint  - Add filters for Inference Job list V1.1.0 2023/7/14 V1.0.1 - Upgrade the version support of webUI to V1.4.0   - Upgrade the version support of ControlNet to V1.1.227   - Upgrade the version support of Dreambooth to V1.0.14    - Support txt2img API Direct call  - Support full features in img2img except batch  - Support full features in ControlNet except multi-ControlNet  - Auto-refresh inference results in Output section  - Support filter in Inference Job ID  - Support rollback or update the middleware  - Bugs fix V1.0.1 2023/6/16 V1.0.0 Initial release V1.0.0"},{"location":"template/","title":"Template","text":"<p>To automated deployment, this solution uses the following AWS CloudFormation templates, which you can download before deployment:</p> <p>[xxx.template]: Use this template to launch the solution and all associated components. The default configuration deploys [Amazon API Gateway][api-gateway], [Amazon Lambda][lambda], [Amazon Batch][Batch], [Amazon S3][s3], [Amazon EFS][efs] and [Amazon Batch][Batch], but you can customize the template to meet your specific needs.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>The following help you to fix errors or problems that you might encounter when using Extension for Stable Diffusion on AWS.</p> <p>Error: It presents error message 'RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'' when I use txt2img inference function</p> <p>The error appears when deploying webUI frontend. It's recommended to start webUI adding <code>--precision full --no-half</code> <pre><code>./webui.sh --skip-torch-cuda-test --precision full --no-half\n</code></pre></p> <p>Error: I cannot install python venv on Ubuntu using <code>python3 -m venv env</code></p> <p>The error comes out when your system's default is Python 3.9, and Ubuntu 20.04 is Python 3.8. User can install venv package using <code>sudo apt install python3.8-venv</code> to explicitly mentioning the full version of Python.</p> <p>Error\uff1a When using the webUI, an Error prompt appears in the top right corner</p> <p>It is likely to be a connection error, indicating that the open-source webUI project has crashed. You can manually restart the webUI to resolve this issue. It's recommended to use the Restart webUI button for manual restart.  </p>"},{"location":"uninstall/","title":"Uninstall Extension for Stable Diffusion on AWS","text":"<p>Warning</p> <p>Before uninstalling the solution, please manually delete all the Amazon SageMaker Endpoints deployed by this solution, referring to Delete deployed endpoint in Main tab. By uninstalling the solution, the DynamoDB tables that indicate the model training, finetuning and inference logs and mapping relationship, AWS Lambda related functions, AWS Step Functions and so on will be deleted simultaneously.</p> <p>To uninstall the Extension for Stable Diffusion on AWS solution, you must delete the AWS CloudFormation stack. </p> <p>You can use either the AWS Management Console or the AWS Command Line Interface (AWS CLI) to delete the CloudFormation stack.</p>"},{"location":"uninstall/#uninstall-the-stack-using-the-aws-management-console","title":"Uninstall the stack using the AWS Management Console","text":"<ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select this solution\u2019s installation parent stack.</li> <li>Choose Delete.</li> </ol>"},{"location":"uninstall/#uninstall-the-stack-using-aws-command-line-interface","title":"Uninstall the stack using AWS Command Line Interface","text":"<p>Determine whether the AWS Command Line Interface (AWS CLI) is available in your environment. For installation instructions, refer to What Is the AWS Command Line Interface in the AWS CLI User Guide. After confirming that the AWS CLI is available, run the following command.</p> <pre><code>aws cloudformation delete-stack --stack-name &lt;installation-stack-name&gt; --region &lt;aws-region&gt;\n</code></pre>"},{"location":"use-cases/","title":"Use cases","text":"<p>"},{"location":"architecture-overview/architecture-details/","title":"Architecture details","text":"<p>Diagram below is the brief view of internal workflow between offered extension and middleware, user will keep launching community WebUI onto standalone EC2/local server with our extension installed, while the ckpt merge, training and inference workload will be migrate onto AWS cloud through the RESTful API provided by middleware installed on user\u2019s AWS account. Note the middleware is per AWS account, means it could be installed separately as work node to communicate with WebUI as control node, user only need to input endpoint URL and API key per account to decide which specific AWS account will be used for successive jobs.</p> <p> Overall Workflow</p> <p>The middleware provides a RESTful API externally to comply with the OpenAPI specification to help WebUI extension to interact with AWS (Amazon SageMaker, S3, etc.). The main functions include request authentication, request distribution (such as SageMaker.jumpstart/model/predictor/estimator/tuner /utils, etc.), model training, model inference and other life cycle management work. The following figure shows the overall architecture of the middleware:</p> <p> Middleware Architecture</p> <ul> <li>Users in the WebUI console will use the assigned API token to trigger a request to API Gateway while being authenticated. (Note: AWS credentials are not required in AWS WebUI)</li> <li>API Gateway will route requests to Lambda with different functions according to URL prefixes to implement corresponding tasks (for example, model uploading, checkpoint merging), model training, and model inference. At the same time, the Lambda function records operational metadata into DynamoDB (eg, inferred parameters, model name) for subsequent query and correlation.</li> <li>During the training process, the Step Function will be called to orchestrate the training process, which includes using Amazon SageMaker for training and SNS for training status notification. During the inference process, the Lambda function will call Amazon SageMaker for asynchronous inference. Training data, models and checkpoints will be stored in S3 buckets separated by different prefixes.</li> </ul> <p>To keep container image of extension in sync with community, additional CI/CD pipeline (fig shown below) may needed to auto track community commits and pack &amp; build new container image, then user can easily launch latest extension without any manual operation.</p> <p> Image CI/CD Workflow</p>"},{"location":"architecture-overview/architecture/","title":"Architecture diagram","text":"<p>The overall architecture of the extension is composed of two components: the extension and the middleware. The extension is a WebUI extension that is installed on the community WebUI and responsible for providing a user interface for users to interact with the middleware. The middleware is a set of AWS resources that are deployed on the user's AWS account and responsible for providing RESTful APIs for the extension to interact with AWS resources. The whole solution provides a seamless experience for users to train and deploy models on AWS with following features:</p> <ul> <li>User Experience: Existing working flow is not changed, user can still use the community WebUI to train and deploy models with third-party extensions.</li> <li>Scalability: Existing workload including training and inference can be easily scaled and accelerated on Amazon SageMaker.</li> <li>Community: A provided extension is part of the community WebUI, which is open source and keeps evolving with the community.</li> </ul>"},{"location":"architecture-overview/design-considerations/","title":"Design considerations","text":"<p>This solution was designed with best practices from the AWS Well-Architected Framework which helps customers design and operate reliable, secure, efficient, and cost-effective workloads in the cloud. </p> <p>This section describes how the design principles and best practices of the Well-Architected Framework were applied when building this solution. </p>"},{"location":"deployment/deployment/","title":"Deploy for SD webUI","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p> <p>Time to deploy: Approximately 20 minutes.</p>"},{"location":"deployment/deployment/#prerequisites","title":"Prerequisites","text":"<p>Users need to prepare a computer-running linux system in advance.</p>"},{"location":"deployment/deployment/#deployment-overview","title":"Deployment overview","text":"<p>Use the following steps to deploy this solution on AWS. </p> <ul> <li>Step 0: Deploy Stable Diffusion webUI (if you haven't deployed Stable Diffusion webUI before). </li> <li>Step 1: Deploy the solution as middleware.</li> <li>Step 2: Configure API url and API token.</li> </ul> <p>Notice</p> <p>This solution provides two usage options: through UI interface and by directly calling the backend API. Step 0 only needs to be executed if the user intends to use the UI interface. This step involves installing another open-source project Stable Diffusion webUI, allowing business operations to be conducted through the webUI.</p>"},{"location":"deployment/deployment/#deployment-steps","title":"Deployment steps","text":""},{"location":"deployment/deployment/#step-0-linux-deploy-stable-diffusion-webui-linux","title":"Step 0 - Linux: Deploy Stable Diffusion WebUI (Linux).","text":"<ol> <li>Sign in to the AWS Management Console and use WebUI on EC2 to create the stack.</li> <li>On the Create Stack page, choose Next.</li> <li>On the Specify stack details page, type a stack name in the Stack name box, adjust parameters as need, then choose Next.</li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review the details of your stack, check capabilities as required, and choose Submit.</li> <li>Wait until the stack is created.</li> <li>Find the output value of the CloudFormation stack, and navigate to the WebUI by clicking the link in the WebUIURL value, note you need to wait an extra 30 minutes to wait for the internal setup complete after the stack been created successfully.</li> </ol>"},{"location":"deployment/deployment/#step-0-windows-deploy-stable-diffusion-webui-windows","title":"Step 0 - Windows: Deploy Stable Diffusion WebUI (Windows).","text":"<ol> <li>Start a Windows Server and log in via RDP.</li> <li>Refer to this link to install the NVIDIA driver.</li> <li>Visit the Python website, download Python, and install it. Remember to check \"Add Python to Path\" during installation.</li> <li>Visit the Git website, download Git, and install it.</li> <li>Open PowerShell and download the source code of this project by executing: <code>git clone https://github.com/awslabs/stable-diffusion-aws-extension</code>.</li> <li>Inside the source code directory, run <code>install.bat</code>.</li> <li>In the downloaded <code>stable-diffusion-webui</code> folder, run <code>webui-user.bat</code>.</li> </ol>"},{"location":"deployment/deployment/#step-1-deploy-the-solution-as-a-middleware","title":"Step 1: Deploy the solution as a middleware.","text":"<p>This automated AWS CloudFormation template deploys the solution in the AWS Cloud.</p> <ol> <li>Sign in to the AWS Management Console and use Launch solution in AWS Standard Regions to launch the AWS CloudFormation template.   </li> <li>The template will launch in the default region when you log into the console by default. To launch this solution in a different AWS Region, use the Region selector in the console navigation bar.</li> <li> <p>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a valid and account level unique name to your solution stack. </p> </li> <li>On the Parameters page, enter a new valid bucket name under Bucket for this solution to use, which is mainly for uploading dates and storing results. Enter a correct email address under email for future notice receiving. Select desired Amazon log level to be printed under LogLevel, only ERROR log will be printed by default. Enter a string of 20 characters that includes a combination of alphanumeric characters for SdExtensionApiKey, and it will be 09876543210987654321 by default, etc. Choose Next.</li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Check the box acknowledging that the template will create AWS Identity and Access Management (IAM) resources. Choose Create stack to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation Console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p> <p>Notice</p> <p>Please check the inbox of the email address you previously set up and click on the \"Confirm subscription\" hyperlink in the email with the subject \"AWS Notification - Subscription Confirmation\" to complete the subscription, and the message of 'Subscription confirmed!' appears.</p>"},{"location":"deployment/deployment/#step2-configure-api-url-and-api-token","title":"Step2: Configure API url and API token.","text":"<p>After successfully stack creation, you can refer to here for subsequent configuration work.</p>"},{"location":"deployment/deployment_comfyui/","title":"Deploy for ComfyUI","text":"<p>Before deploying the solution, it is recommended that you first review information in this guide regarding architecture diagrams and regional support. Then, follow the instructions below to configure the solution and deploy it to your account.</p> <p>Deployment time: arount 20 minutes.</p>"},{"location":"deployment/deployment_comfyui/#deployment-summary","title":"Deployment Summary","text":"<p>Deploying this solution (ComfyUI portion) on Amazon Web Services primarily involves the following processes:</p> <ul> <li>Step 1: Deploy the middleware of the solution.</li> <li>Step 2: Deploy ComfyUI frontend.</li> </ul> <p>Tip</p> <pre><code>You can refer to the ComfyUI section in the FAQ chapter if you encounter deployment issues.\n</code></pre>"},{"location":"deployment/deployment_comfyui/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/deployment_comfyui/#step-1-deploy-the-middleware-of-the-solution","title":"Step 1: Deploy the middleware of the solution","text":"<p>This automated Amazon CloudFormation template deploys the solution in Amazon Web Services.</p> <ol> <li>Sign in to the AWS Management Console\uff0cand use Extension for Stable Diffusion on AWS to create the stack.</li> <li>By default, this template will launch in the default region after you log in to the console. To launch this solution in a specified Amazon Web Services region, please select the desired region from the region drop-down list in the console's navigation bar.</li> <li>On the Create Stack page\uff0cconfirm that the correct template URL has been entered in the Amazon S3 URL text box, then select Next.</li> <li> <p>On the Specify stack details page, assign a unique name within your account that meets the naming requirements for your solution stack. Refer to the table below for deployment parameters. Click Next.</p> Parameter Description Recommendation Bucket Enter a valid new S3 bucket name (or the name of a previously deployed S3 bucket used for the ComfyUI section of this solution) email Enter a valid email address for further notification receivement SdExtensionApiKey \u8bf7\u8f93\u5165\u4e00\u4e2a\u5305\u542b\u6570\u5b57\u548c\u5b57\u6bcd\u7ec4\u5408\u768420\u4e2a\u5b57\u7b26\u7684\u5b57\u7b26\u4e32 \u9ed8\u8ba4\u4e3a\"09876543210987654321\" LogLevel \u62e9\u60a8\u5fc3\u4eea\u7684Lambda Log\u65e5\u5fd7\u6253\u5370\u7ea7\u522b \u9ed8\u8ba4ERROR\u624d\u6253\u5370 </li> <li> <p>\u5728\u914d\u7f6e\u5806\u6808\u9009\u9879\u9875\u9762\uff0c\u9009\u62e9\u4e0b\u4e00\u6b65\u3002</p> </li> <li>\u5728\u5ba1\u6838\u9875\u9762\uff0c\u67e5\u770b\u5e76\u786e\u8ba4\u8bbe\u7f6e\u3002\u786e\u4fdd\u9009\u4e2d\u786e\u8ba4\u6a21\u677f\u5c06\u521b\u5efaAmazon Identity and Access Management\uff08IAM\uff09\u8d44\u6e90\u7684\u590d\u9009\u6846\u3002\u5e76\u786e\u4fdd\u9009\u4e2dAWS CloudFormation\u9700\u8981\u7684\u5176\u5b83\u529f\u80fd\u7684\u590d\u9009\u6846\u3002\u9009\u62e9\u63d0\u4ea4\u4ee5\u90e8\u7f72\u5806\u6808\u3002</li> <li> <p>\u60a8\u53ef\u4ee5\u5728 AWS CloudFormation \u63a7\u5236\u53f0\u7684 \u72b6\u6001 \u5217\u4e2d\u67e5\u770b\u5806\u6808\u7684\u72b6\u6001\u3002\u60a8\u5e94\u8be5\u4f1a\u5728\u5927\u7ea6 15 \u5206\u949f\u5185\u6536\u5230CREATE_COMPLETE\u72b6\u6001\u3002</p> <p>\u8d34\u58eb</p> <p>\u8bf7\u53ca\u65f6\u68c0\u67e5\u60a8\u9884\u7559\u90ae\u7bb1\u7684\u6536\u4ef6\u7bb1\uff0c\u5e76\u5728\u4e3b\u9898\u4e3a\u201cAWS Notification - Subscription Confirmation\u201d\u7684\u90ae\u4ef6\u4e2d\uff0c\u70b9\u51fb\u201cConfirm subscription\u201d\u8d85\u94fe\u63a5\uff0c\u6309\u63d0\u793a\u5b8c\u6210\u8ba2\u9605\u3002</p> </li> </ol>"},{"location":"deployment/deployment_comfyui/#2-comfyui","title":"\u6b65\u9aa42: \u90e8\u7f72ComfyUI\u524d\u7aef","text":"<p>\u6b65\u9aa42\u5c06\u4f1a\u4e3a\u5ba2\u6237\u5b89\u88c5ComfyUI\u7684\u524d\u7aef\u3002\u8be5\u524d\u7aef\u81ea\u52a8\u5185\u7f6e\u4e86\u6c49\u5316\u63d2\u4ef6\u3001\u5de5\u4f5c\u6d41\u53d1\u5e03\u4e91\u4e0a\u7b49\u6309\u94ae\uff0c\u4e3a\u5ba2\u6237\u63d0\u4f9b\u66f4\u53cb\u597d\u7684UI\u4ea4\u4e92\u754c\u9762\u3002\u6b64\u81ea\u52a8\u5316Amazon CloudFormation\u6a21\u677f\u5728\u4e9a\u9a6c\u900a\u4e91\u79d1\u6280\u4e2d\u90e8\u7f72\u3002</p> <ol> <li>\u767b\u5f55\u5230AWS\u7ba1\u7406\u63a7\u5236\u53f0\uff0c\u70b9\u51fb\u63a7\u5236\u53f0\u53f3\u4e0a\u89d2Create Stack, With new resource(standard)\uff0c\u9875\u9762\u8df3\u8f6c\u81f3\u521b\u5efa\u5806\u6808\u3002</li> <li>\u5728\u521b\u5efa\u5806\u6808\u9875\u9762\u4e0a\uff0c\u9009\u62e9Choose an existing template\uff0c\u5728\u7279\u5b9a\u6a21\u7248\u533a\u57df\u9009\u62e9Amazon S3 URLe\uff0c\u586b\u5165\u8be5\u90e8\u7f72\u6a21\u7248\u94fe\u63a5\uff0c\u7136\u540e\u9009\u62e9\u4e0b\u4e00\u6b65\u3002</li> <li> <p>\u5728\u5236\u5b9a\u5806\u6808\u8be6\u7ec6\u4fe1\u606f\u9875\u9762\uff0c\u4e3a\u60a8\u7684\u89e3\u51b3\u65b9\u6848\u5806\u6808\u5206\u914d\u4e00\u4e2a\u8d26\u6237\u5185\u552f\u4e00\u4e14\u7b26\u5408\u547d\u540d\u8981\u6c42\u7684\u540d\u79f0\u3002\u5728\u53c2\u6570\u90e8\u5206\uff0c\u90e8\u7f72\u53c2\u6570\u8bf4\u660e\u5982\u4e0b\u3002\u70b9\u51fbNext\u3002</p> <p>\u8d34\u58eb</p> <p>\u6b64\u5904\u7684EC2 Key Pair\u4e3b\u8981\u7528\u4e8e\u672c\u5730\u8fdc\u7a0b\u8fde\u63a5EC2\u3002\u5982\u679c\u6ca1\u6709\u73b0\u6709\u7684\uff0c\u53ef\u4ee5\u53c2\u8003\u5b98\u65b9\u624b\u518c\u6765\u521b\u5efa\u3002</p> \u53c2\u6570 \u8bf4\u660e \u5efa\u8bae InstanceType \u90e8\u7f72\u7684ec2\u7684\u5b9e\u4f8b\u7c7b\u578b \u5982\u679c\u662f\u6d89\u53ca\u63a8\u7406\u52a8\u56fe\u3001\u89c6\u9891\u7b49\uff0c\u5efa\u8baeG6\u3001G5\u673a\u5668 NumberOfInferencePorts \u63a8\u7406\u73af\u5883\u6570\u91cf \u5efa\u8bae\u4e0d\u8d85\u8fc75\u4e2a StackName \u6765\u81ea\u4e8e\u90e8\u7f72\u6b65\u9aa41\u4e2d\u6210\u529f\u90e8\u7f72\u5806\u6808\u7684\u540d\u79f0 keyPairName \u9009\u62e9\u73b0\u6709\u7684\u4e00\u4e2aEC2 Key Pair </li> <li> <p>\u5728\u914d\u7f6e\u5806\u6808\u9009\u9879\u9875\u9762\uff0c\u9009\u62e9\u4e0b\u4e00\u6b65\u3002</p> </li> <li>\u5728\u5ba1\u6838\u9875\u9762\uff0c\u67e5\u770b\u5e76\u786e\u8ba4\u8bbe\u7f6e\u3002\u786e\u4fdd\u9009\u4e2d\u786e\u8ba4\u6a21\u677f\u5c06\u521b\u5efaAmazon Identity and Access Management\uff08IAM\uff09\u8d44\u6e90\u7684\u590d\u9009\u6846\u3002\u5e76\u786e\u4fdd\u9009\u4e2dAWS CloudFormation\u9700\u8981\u7684\u5176\u5b83\u529f\u80fd\u7684\u590d\u9009\u6846\u3002\u9009\u62e9\u63d0\u4ea4\u4ee5\u90e8\u7f72\u5806\u6808\u3002</li> <li>\u60a8\u53ef\u4ee5\u5728 AWS CloudFormation \u63a7\u5236\u53f0\u7684 \u72b6\u6001 \u5217\u4e2d\u67e5\u770b\u5806\u6808\u7684\u72b6\u6001\u3002\u60a8\u5e94\u8be5\u4f1a\u5728\u5927\u7ea6 3 \u5206\u949f\u5185\u6536\u5230CREATE_COMPLETE\u72b6\u6001\u3002</li> <li> <p>\u9009\u62e9\u90e8\u7f72\u6210\u529f\u7684\u5806\u6808\uff0c\u6253\u5f00Outputs\uff0c\u70b9\u51fbDesigner\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u6253\u5f00\u89e3\u51b3\u65b9\u6848\u90e8\u7f72\u7684ComfyUI\u524d\u7aef\uff0cDesigner\u7684\u8bbf\u95ee\u53ef\u80fd\u9700\u8981\u5173\u95edVPN\u6216\u8005\u53bb\u638910000\u7aef\u53e3\u540e\u8bbf\u95ee\u3002NumberOfInferencePortsStart\u4ee3\u8868\u63a8\u7406\u73af\u5883\u5730\u5740\u8d77\u59cb\u8def\u5f84\u7aef\u53e3\uff0c\u6309\u7167\u90e8\u7f72\u6570\u91cf\u7aef\u53e3\u5730\u5740\u4f9d\u6b21\u589e\u52a0\uff0c\u4f8b\u5982\uff1a\u5f53NumberOfInferencePorts\u586b\u51992\u65f6\uff0c\u5730\u5740\u8303\u56f4\u65f6\uff0c\u53ef\u8bbf\u95ee\u7684\u63a8\u7406\u73af\u5883\u5730\u5740\u4f9d\u6b21\u4e3a\uff1ahttp://EC2\u5730\u5740:10001\uff0chttp://EC2\u5730\u5740:10002.</p> \u89d2\u8272 \u529f\u80fd \u7aef\u53e3 \u4e3b\u7f8e/\u5de5\u4f5c\u6d41\u7ba1\u7406 \u80fd\u591f\u5b89\u88c5\u65b0\u7684custom nodes\uff0c\u5728EC2\u4e0a\u8c03\u8bd5\u5de5\u4f5c\u6d41\uff0c\u53d1\u5e03\u5de5\u4f5c\u6d41\u3001\u73af\u5883\u81f3Amazon SageMaker\u3002\u540c\u65f6\u53ef\u4ee5\u8c03\u7528SageMaker\u8d44\u6e90\u3001\u9009\u4e2d\u5df2\u53d1\u5e03\u7684\u5de5\u4f5c\u6d41\u8fdb\u884c\u63a8\u7406\u9a8c\u8bc1 http://EC2\u5730\u5740 \u666e\u901a\u7f8e\u672f \u4ece\u8be5\u7aef\u53e3\u8fdb\u5165\u7684\u754c\u9762\uff0c\u53ef\u4ee5\u9009\u62e9\u4e3b\u7f8e\u5df2\u53d1\u5e03\u7684\u5de5\u4f5c\u6d41\uff0c\u7b80\u5355\u4fee\u6539\u63a8\u7406\u53c2\u6570\u540e\uff0c\u52fe\u9009\u201cPrompt on AWS\u201d\u540e\u3001\u8c03\u7528Amazon SageMaker\u8fdb\u884c\u63a8\u7406 \u5f53NumberOfInferencePorts\u586b\u51993\u65f6\uff0c\u5730\u5740\u8303\u56f4\u65f6\uff0c\u53ef\u8bbf\u95ee\u7684\u63a8\u7406\u73af\u5883\u5730\u5740\u4f9d\u6b21\u4e3a\uff1a<ul><li>http://EC2\u5730\u5740:10001 </li><li>http://EC2\u5730\u5740:10002 </li><li>http://EC2\u5730\u5740:10003</li></ul> <p>\u8d34\u58eb</p> <p>\u521a\u90e8\u7f72\u597d\u8d34\u58eb\u4ee5\u540e\uff0c\u9700\u8981\u7a0d\u4f5c\u7b49\u5f85\u3002\u5982\u679c\u6253\u5f00\u94fe\u63a5\u540e\uff0c\u770b\u5230\u63d0\u793a\u201cComfy is Initializing or Starting\u201d\uff0c\u8868\u793a\u540e\u7aef\u5728\u521d\u59cb\u5316ComfyUI\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u7a0d\u4f5c\u7b49\u5f85\uff0c\u518d\u6b21\u5237\u65b0\u9875\u9762\u786e\u8ba4\u3002</p> </li> </ol>"},{"location":"deployment/deployment_comfyui/#step3-comfyui","title":"Step3: \u5728ComfyUI\u9875\u9762\u8c03\u8bd5\u5e76\u521b\u5efa\u4e00\u4e2a\u53ef\u7528\u7684\u5de5\u4f5c\u6d41\u3002","text":"<p>\u53ef\u4ee5\u53c2\u8003\u8fd9\u91cc\u4e2d\u7684\u201c\u5de5\u4f5c\u6d41\u7684\u8c03\u8bd5\u201d\u5b50\u7ae0\u8282\u90e8\u5206</p>"},{"location":"deployment/deployment_comfyui/#step-4-deploy-new-amazon-sagemaker-inference-endpoint","title":"Step 4: Deploy new Amazon SageMaker inference endpoint","text":"<p>After successfully completing step 1, you need to deploy the required Amazon SageMaker inference nodes using API. Subsequent deployments of new ComfyUI workflow inferences will utilize the computational resources of these inference nodes.</p> <p>The <code>ApiGatewayUrl</code> and <code>ApiGatewayUrlToken</code> required in the following API code can be found in the Outputs tab of the stack deployed successfully in step 1.</p> <p>Please open any command-line interface capable of running code, such as Terminal on a local MacBook, and execute the following API code.</p> <pre><code>curl --location \u2018YourAPIURL/endpoints\u2019 \\\n--header \u2018x-api-key: Your APIkey\u2019 \\\n--header \u2018username: api\u2019 \\\n--header \u2018Content-Type: application/json\u2019 \\\n--data-raw \u2018{\n    \u201cworkflow_name\u201d:\u201cPlease fill the name of template you just released\u201c,\n    \u201cendpoint_name\u201d: \u201cWhen you don't need to associate it with a workflow, you should fill in the name of the inference endpoint you want to create\",\n    \u201cservice_type\u201d: \u201ccomfy\u201d,\n    \u201cendpoint_type\u201d: \u201cAsync\u201d,\n    \u201cinstance_type\u201d: \u201cinstance type\u201d,\n    \u201cinitial_instance_count\u201d: 1,\n    \u201cmin_instance_number\u201d: 1,\n    \u201cmax_instance_number\u201d: 2,\n    \u201cautoscaling_enabled\u201d: true,\n    \u201cassign_to_roles\u201d: \u201ctest\u201d\n    \u201cassign_to_roles\u201d: [ \u201ctest\u201d ]\n}\u2019\n</code></pre> <p>Important</p> <p>If your workflow is relatively complex, it's important to select asynchronous inference node types. Otherwise, you may encounter timeout issues due to the service's maximum wait time of 30 seconds for synchronous calls.</p> <p>Delete corresponding Amazon SageMaker endpoint, can be executed as below: <pre><code>curl --location --request DELETE 'https://please fill ApiGatewayUrl/endpoints' \\\n--header 'username: api' \\\n--header 'x-api-key: please type the ApiGatewayUrlToken' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"endpoint_name_list\": [\n        \"comfy-real-time-test-34\"//type the name of the endpoint\n    ]\n}'\n</code></pre></p> <p>Important</p> <p>It's not recommended to directly delete endpoints from the SageMaker console as it can potentially lead to inconsistencies in data.</p>"},{"location":"deployment/deployment_for_existing_users/","title":"Update SD webUI","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p> <p>Time to deploy: Approximately 20 minutes.</p>"},{"location":"deployment/deployment_for_existing_users/#prerequisites","title":"Prerequisites","text":"<ul> <li>The user needs to prepare a computer running a Linux system in advance.</li> <li>Install and configure aws cli.</li> <li>Deploy the previous version of the Stable Diffusion Webui AWS plugin.</li> </ul>"},{"location":"deployment/deployment_for_existing_users/#deployment-overview","title":"Deployment overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <ul> <li>Step 1: Update Stable Diffusion WebUI.</li> <li>Step 2: After logging into the AWS Console, update the existing Stable Diffusion AWS extension template in CloudFormation.</li> </ul>"},{"location":"deployment/deployment_for_existing_users/#deployment-steps","title":"Deployment steps","text":""},{"location":"deployment/deployment_for_existing_users/#step-1-linuxupdate-stable-diffusion-webui-linux","title":"Step 1 - Linux\uff1aUpdate Stable Diffusion WebUI (Linux).","text":"<ol> <li> <p>Download the CloudFormation Template from link</p> </li> <li> <p>Sign in to the AWS Management Console and go to CloudFormation console</p> </li> <li> <p>On the Stacks page, choose Create stack, and then choose With new resources (standard).</p> </li> <li> <p>On the Specify template page, choose Template is ready, choose Upload a template file, and then browse for the template that is downloaded in step 1, and then choose Next.</p> </li> <li> <p>On the Specify stack details page, type a stack name in the Stack name box, then choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review the details of your stack, and choose Submit.</p> </li> <li> <p>Wait until the stack is created.</p> </li> <li> <p>Find the output value of the CloudFormation stack, and navigate to the WebUI by clicking the link in the WebUIURL value, note you need to wait extra 30 minutes to wait for the internal setup complete after the stack been created successfully.</p> </li> </ol>"},{"location":"deployment/deployment_for_existing_users/#step-1-windows-update-stable-diffusion-webui-windows","title":"Step 1 - Windows: Update Stable Diffusion WebUI (Windows).","text":"<ol> <li>Start a Windows Server and log in via RDP.</li> <li>Refer to this link to install the NVIDIA driver.</li> <li>Visit the Python website, download Python, and install it. Remember to check \"Add Python to Path\" during installation.</li> <li>Visit the Git website, download Git, and install it.</li> <li>Open PowerShell and download the source code of this project by executing: <code>git clone https://github.com/awslabs/stable-diffusion-aws-extension</code>.</li> <li>Inside the source code directory, run <code>install.bat</code>.</li> <li>In the downloaded <code>stable-diffusion-webui</code> folder, run <code>webui-user.bat</code>.</li> </ol>"},{"location":"deployment/deployment_for_existing_users/#step-2update-the-existing-stable-diffusion-aws-extension-template-in-cloudformation","title":"Step 2\uff1aUpdate the existing Stable Diffusion AWS extension template in CloudFormation.","text":"<ol> <li>Open the AWS Management Console (https://console.aws.amazon.com) and log in.</li> <li>Select \"CloudFormation\" from the service menu, find the stack deployed for this solution, and select it, click Update in the upper right.</li> <li>In Update Stack, select Replace current template, enter latest CloudFormation link in Amazon S3 URL and click Next.</li> <li>In Configure stack options, click Next.</li> <li>In Review, select acknowledge option and click Submit. </li> <li>CloudFormation will start updating the stack, which may take some time. You can monitor the status of the stack on the Stacks page.</li> </ol>"},{"location":"deployment/deployment_for_existing_users/#notice","title":"Notice","text":"<ol> <li>SageMaker Inference Endpoints need to delete and deploy new after updating solution to V1.5.0</li> <li>The version of webUI and middleware should match.</li> <li>Recommend to deploy new webUI on EC2. </li> <li>Middleware API version 1.4.0 can be updated directly, while version 1.3.0 needs to be uninstalled first and then reinstalled.</li> <li>If it involves services already integrated through an API, please review the API documentation for upgrading permission verification methods, and conduct thorough testing before going live.</li> </ol>"},{"location":"deployment/permissions/","title":"Permissions","text":"<p>When deploying, updating, and managing the resources deployed in this solution, it is recommended to follow the minimum permission principle to grant permissions to the monitored account.</p> <p>The permissions required for deploying, updating, and managing the resources deployed in this solution are as follows:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"apigateway:*\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"cloudformation:*\",\n                \"cloudwatch:DeleteAlarms\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:PutMetricData\",\n                \"dynamodb:*\",\n                \"ecr:*\",\n                \"events:*\",\n                \"iam:*\",\n                \"kms:*\",\n                \"lambda:*\",\n                \"logs:*\",\n                \"s3:*\",\n                \"sagemaker:*\",\n                \"sns:*\",\n                \"states:*\",\n                \"sts:AssumeRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n</code></pre>"},{"location":"deployment/template/","title":"AWS CloudFormation template","text":"<p>To automated deployment, this solution uses the following AWS CloudFormation templates, which you can download before deployment:</p> <p>Extension for Stable Diffusion on AWS: Use this template to launch the solution and all associated components. The default configuration deploys Amazon API Gateway, AWS Lambda, Amazon S3, Amazon SageMaker and Amazon DynamoDB, but you can customize the template to meet your specific needs.</p>"},{"location":"developer-guide/access/","title":"API Access Restriction","text":"<p>The API is open to the public network and the type is <code>REGIONAL</code> by default. </p> <p>If you want to restrict access to the API, you can set <code>ApiEndpointType</code> to <code>PRIVATE</code>.</p> <p>You can link VpcEndpoint to the API by setting <code>VpcEndpointIds</code> to the VPC endpoint ID.</p>"},{"location":"developer-guide/api/","title":"ESD API","text":""},{"location":"developer-guide/api_authentication/","title":"API Authentication","text":""},{"location":"developer-guide/api_authentication/#security-authentication","title":"Security Authentication","text":"<p>All APIs use API keys for security verification, and all API requests should include your API key in the HTTP header. The <code>x-api-key</code> is as follows: <pre><code>X-api-key: xxxxxxxxxxxxxxxxxxxx\n</code></pre></p>"},{"location":"developer-guide/api_authentication/#user-authentication","title":"User Authentication","text":"<p>Please include <code>username</code> in the HTTP header. For example, if the username configured on WebUI is <code>admin</code>, then:</p> <pre><code>username: admin\n</code></pre> <p>After the API is deployed, a user named <code>api</code> is built in. If you do not use the WebUI for initialization or create a new user through the API, you can use <code>api</code> as the username.</p>"},{"location":"developer-guide/api_authentication/#version-140-or-earlier","title":"Version 1.4.0 or earlier","text":"<p>Your <code>Authorization</code> should be included in the HTTP header as follows:</p> <pre><code>Authorization: Bearer {TOKEN}\n</code></pre> <p>Token algorithm (Python example):</p> <pre><code>Import base64\nUsername=\"your username on webui\"\nToken=base64.b16encode (username. encode (\"utf-8\")). decode (\"utf-8\")\n</code></pre> <p>For example, if the username configured on WebUI is <code>admin</code>, then: <pre><code>Authorization: Bearer 61646D696E\n</code></pre></p>"},{"location":"developer-guide/api_debugger/","title":"API Inference Debugger","text":"<p>Due to the numerous argument variables involved, it is recommended to use <code>API Inference Debugger</code> to assist with the development of inference tasks. The <code>API Inference Debugger</code> will fully record the actual inference process and parameters. You only need to copy the relevant data structures and make minor modifications.</p>"},{"location":"developer-guide/api_debugger/#enable-api-inference-debugger","title":"Enable API Inference Debugger","text":"<p>If your WebUI interface does not display the API debugger button shown in the figure below, you may need to update the WebUI plugin code. Please switch to the plugin directory and execute the following command:</p> <p><pre><code>cd /home/ubuntu/stable-diffusion-webui/extensions/stable-diffusion-aws-extension\ngit checkout main\ngit pull\nsudo systemctl restart sd-webui\n</code></pre> Wait for the WebUI to restart and complete within approximately 3 minutes.</p>"},{"location":"developer-guide/api_debugger/#use-api-inference-debugger","title":"Use API Inference Debugger","text":"<p>After completing an inference job, open the API request record in the following order:</p> <ol> <li>Click the button to refresh the inference history job list</li> <li>Pull down the inference job list, find and select the job</li> <li>Click the <code>API</code> button on the right</li> </ol> <p></p>"},{"location":"developer-guide/api_debugger/#api-inference-debugger-log","title":"API Inference Debugger Log","text":"<p>Note: Due to Postman sending requests with its own information, it may result in signature errors. It is recommended to directly use code for requests or refer to the API information provided on the UI.</p>"},{"location":"developer-guide/api_inference_process/","title":"API Inference Process","text":""},{"location":"developer-guide/api_inference_process/#how-to-inference-on-stable-diffusion-through-api","title":"How-to inference on Stable Diffusion through API","text":"<ul> <li>Create Endpoint through <code>CreateEndpoint</code></li> <li>Upload model file through <code>CreateCheckpoint</code>, Please refer to: <code>API Upload Checkpoint Process</code></li> <li>Select <code>Async inference</code> or <code>Real-time inference</code></li> </ul>"},{"location":"developer-guide/api_inference_process/#async-inference","title":"Async inference","text":"<ul> <li>Create an inference job through <code>CreateInferenceJob</code></li> <li>Based on the presigned address <code>api_params_s3_upload_url</code> returned by <code>CreatInferenceJob</code> Upload inference parameters</li> <li>Start an inference job through <code>StartInferenceJob</code></li> <li>Get an inference job through <code>GetInferenceJob</code>, check the status, and stop the request if successful</li> </ul>"},{"location":"developer-guide/api_inference_process/#real-time-inference","title":"Real-time inference","text":"<ul> <li>Create an inference job through <code>CreateInferenceJob</code></li> <li>Based on the pre signed address <code>api_params_s3_upload_url</code> returned by <code>CreatInferenceJob</code> Upload inference parameters</li> <li>Starting the inference job through <code>StartInferenceJob</code>, the real-time inference job will get the inference result in this interface</li> </ul>"},{"location":"developer-guide/api_inference_process/#how-to-inference-on-comfyui-through-api","title":"How-to inference on ComfyUI through API","text":""},{"location":"developer-guide/api_inference_process/#async-inference_1","title":"Async inference","text":"<ul> <li>Create Endpoint through <code>CreateEndpoint</code></li> <li>Create an inference job through <code>CreateExecute</code></li> <li>Get an inference job through <code>GetExcute</code>, check the status, and stop the request if successful</li> </ul>"},{"location":"developer-guide/api_upload_ckpt/","title":"API Upload Checkpoint Process","text":""},{"location":"developer-guide/api_upload_ckpt/#upload-through-url","title":"Upload through URL","text":"<ul> <li>Request <code>CreateCheckpoint</code></li> </ul>"},{"location":"developer-guide/api_upload_ckpt/#upload-through-file","title":"Upload through file","text":"<ul> <li>Request <code>CreateCheckpoint</code> to create a model file</li> <li>Upload files through S3 pre signed address</li> <li>Update status through <code>UpdateCheckpoint</code></li> </ul>"},{"location":"developer-guide/byoc/","title":"Custom Container","text":""},{"location":"developer-guide/byoc/#stable-diffusion-train-and-deploy-api","title":"Custom Container","text":""},{"location":"developer-guide/byoc/#overview","title":"Overview","text":"<p>The Extension for Stable Diffusion on AWS is extremely flexible. You can replace the container image of the SageMaker Endpoint model at any time.</p> <p>To achieve this capability, follow these steps:</p> <ul> <li>Step 1: Build a Container Image</li> <li>Step 2: Create Endpoint using custom container image</li> <li>Step 3: Verify or diagnose whether the container image is work</li> </ul> <p></p>"},{"location":"developer-guide/byoc/#build-a-container-image","title":"Build a Container Image","text":"<p>You can build your own container image and upload it to Amazon ECR in the region where the solution is deployed, please read Using Amazon ECR with the AWS CLI , after the operation is complete, you will get a ECR URI, such as:</p> <pre><code>{your_account_id}.dkr.ecr.{region}.amazonaws.com/your-image:latest\n</code></pre> <p>Dockerfile template:</p> <pre><code># It is recommended to use the Image created by the solution as the base image.\nFROM {your_account_id}.dkr.ecr.{region}.amazonaws.com/stable-diffusion-aws-extension/aigc-webui-inference:latest\n\n# Download the extension\nRUN mkdir -p /opt/ml/code/extensions/ &amp;&amp; \\\n    cd /opt/ml/code/extensions/ &amp;&amp; \\\n    git clone https://github.com/**.git\n</code></pre> <p></p>"},{"location":"developer-guide/byoc/#create-endpoint-using-custom-container-image","title":"Create Endpoint using custom container image","text":"<p>Create a role named <code>byoc</code> and add the logged-in user to that role to activate the function shown in the following picture:</p> <p></p> <p></p>"},{"location":"developer-guide/byoc/#verify-or-diagnose-whether-the-container-image-is-work","title":"Verify or diagnose whether the container image is work","text":"<p>After the container image is replaced, you can verify whether the container image is working properly by viewing the logs of the SageMaker Endpoint, or diagnose the cause of the problem:</p> <ul> <li>{region}: The region where the solution is deployed, such as: <code>us-east-1</code></li> <li>{endpoint-name}: Endpoint name, such as: <code>esd-type-111111</code></li> </ul> <pre><code>https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups$3FlogGroupNameFilter$3D{endpoint-name}\n</code></pre>"},{"location":"developer-guide/source/","title":"Source code","text":"<p>Visit our GitHub repository to download the templates and scripts for this solution. The Extension for Stable Diffusion on AWS template is generated using the AWS Cloud Development Kit (CDK). Refer to the README.md file for additional information.</p>"},{"location":"plan-deployment/quotas/","title":"Quotas","text":"<p>To increase the quota of the Amazon SageMaker resources in your AWS account, you can follow these steps:</p> <ol> <li>Log in to the AWS Management Console and navigate to the Service Quotas console.</li> <li>Select AWS services from the navigation panel.</li> <li>Select Amazon SageMaker from the list, or type the service name in the search box.</li> <li>Request a quota increase: If the quota is adjustable, you can select the button or name, then select Request quota increase.</li> <li>Change the quota value: Enter the new value in the Change quota value field. The new value must be greater than the current value.</li> <li> <p>Submit the request: Click Request to submit your quota increase request to AWS.</p> <p>Notice</p> <p>Please note that some AWS services may only be available in certain regions. If you have quota increase requests in different regions, make sure to first select the appropriate region.</p> </li> </ol> <p>After you submit a quota increase request, you can track its status:</p> <ol> <li>View the request status: Return to the Service Quotas console and select Dashboard from the navigation panel. For pending requests, select the status of the request to open the request receipt. The initial status of the request is Pending. When the status changes to Quota Requested, you will see a case number at AWS Support. Select the case number to open your request receipt.</li> <li> <p>Check the quota request history: To view any pending or recently resolved requests, select Quota request history from the Service Quotas console navigation panel.</p> <p>Notice</p> <p>Kindly be noticed that quota increase requests do not receive priority support and may take some time to process. If you have an urgent request, consider contacting AWS Support directly.</p> </li> </ol> <p>Please refer to the following AWS documents for more information about quotas:</p> <ul> <li>Amazon SageMaker Service Quotas</li> <li>Requesting a Quota Increase</li> </ul>"},{"location":"plan-deployment/regions/","title":"Supported regions","text":"<p>As of June 2023, this solution is supported in the following Amazon Web Services Regions:</p> <ul> <li>us-east-1 (Virginia)</li> <li>us-east-2 (Ohio)  </li> <li>us-west-1 (N. California)</li> <li>us-west-2 (Oregon)  </li> <li>ca-central-1 (Canada) </li> <li>sa-east-1 (Sao Paulo)</li> <li>eu-west-1 (Ireland)</li> <li>eu-west-2 (London)</li> <li>eu-west-3 (Paris)   </li> <li>eu-central-1 (Frankfurt)  </li> <li>eu-north-1 (Stockholm)</li> <li>ap-northeast-1 (Tokyo) </li> <li>ap-northeast-2 (Seoul)  </li> <li>ap-northeast-3 (Osaka)</li> <li>ap-southeast-1 (Singapore)  </li> <li>ap-southeast-2 (Sydney)   </li> <li>ap-south-1 (Mumbai)  </li> <li>ap-east-1 (Hong Kong)</li> </ul> <p>Notice</p> <p>Recently, it's observed that newly created Amazon S3 bucket, in us-east-2, us-west-1, us-west-2, there is an issue with CORS that prevents users from uploading configuration files through the browser. Despite updating the CORS configuration, users frequently encounter CORS issues when uploading files using pre-signed URLs. The problem resolves itself after approximately two hours. We are currently in communication with the Amazon S3 Service team regarding this issue. According to that, it's recommended to deploy the solution in us-east-1, ap-northeast-1 or ap-southeast-1.</p>"},{"location":"plan-deployment/security/","title":"Security","text":"<p>When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, visit AWS Cloud Security.</p>"},{"location":"plan-deployment/security/#iam-roles","title":"IAM roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s access between the solution components.</p>"},{"location":"plan-deployment/security/#security-groups","title":"Security groups","text":"<p>The security groups created in this solution are designed to control and isolate network traffic between the solution components. We recommend that you review the security groups and further restrict access as needed once the deployment is up and running.</p>"},{"location":"solution-overview/concepts/","title":"Concepts","text":"<p>This section describes key concepts and defines terminology specific to this solution: </p>"},{"location":"solution-overview/features-and-benefits/","title":"Features and benefits","text":""},{"location":"solution-overview/features-and-benefits/#features","title":"Features","text":"<p>This solution supports the cloud-based operations of the following 3 projects.</p> Project Supported Version Note Stable Diffusion WebUI V 1.8.0 The default supported native/third-party extensions are listed in the table below. ComfyUI 605e64f6d3da44235498bf9103d7aab1c95ef211 The custom nodes that require cloud-based inference support can be packaged and uploaded to the cloud in one click using the template publishing feature provided by this solution. Therefore, this solution does not include built-in support for custom nodes; users can flexibly choose to install and package them for upload. Kohya_ss V0.8.3 Support LoRa model training based on SD 1.5 and SDXL. <p>Below please find the native features/third-party extensions supported by this solution for Stable Diffusion WebUI. Other extensions can be supported through BYOC (Bring Your Own Container).</p> Feature Supported Version Note Stable Diffusion WebUI V1.8.0 Support LCM as official sampler, SDXL-Inpaint, etc img2img V1.8.0 Support all features except batch txt2img V1.8.0 LoRa V1.2.1 ControlNet V1.1.410 Support SDXL + ControlNet Inference Tiled Diffusion &amp; VAE f9f8073e64f4e682838f255215039ba7884553bf ReActor for Stable Diffusion 0.6.1 Extras V1.8.0 API rembg 3d9eedbbf0d585207f97d5b21e42f32c0042df70 API kohya_ss"},{"location":"solution-overview/features-and-benefits/#benefits","title":"Benefits","text":"<ul> <li> <p>Convenient Installation: This solution leverages CloudFormation for easy deployment of AWS middleware. Combined with the installation of the native Stable Diffusion WebUI (WebUI) features and third-party extensions, users can quickly utilize Amazon SageMaker's cloud resources for inference, training and finetuning tasks.</p> </li> <li> <p>Community Native: This solution is implemented as an extension, allowing users to seamlessly use their existing WebUI without any changes. Additionally, the solution's code is open source and follows a non-intrusive design, enabling users to keep up with community-related feature iterations, such as popular plugins like ControlNet, and LoRa.</p> </li> <li> <p>High Scalability: This solution decouples the WebUI interface from the backend, allowing the WebUI to launch on supported terminals without GPU restrictions. Existing training, inference, and other tasks can be migrated to Amazon SageMaker through the provided extension functionalities, providing users with elastic computing resources, cost reduction, flexibility, and scalability.</p> </li> </ul>"},{"location":"solution-overview/use-cases/","title":"Use cases","text":"<p>"},{"location":"user-guide/training-guide/","title":"Training Guide","text":"<p>The training is based on Kohya-SS. Kohya-SS is a Python library for finetuning stable diffusion model which is friendly for consumer-grade GPU and compatible with the Stable Diffusion WebUI. The solution can do LoRA training both on SDXL and SD 1.5.</p>"},{"location":"user-guide/training-guide/#training-user-guide","title":"Training User Guide","text":""},{"location":"user-guide/training-guide/#prepare-foundation-model","title":"Prepare Foundation Model","text":"<p>Please refresh and check the dropdown list under Model to ensure that the required base models for this training session are available. If not, you can use the Upload Models in Cloud Asset Management to upload the base models under the SD Checkpoints category.</p> <p>Alternatively, you can also upload local SD models to an S3 bucket using the following command:</p> <p>Upload your local SD model to S3 bucket by following commands <pre><code># Configure credentials\naws configure\n# Copy local SD model to S3 bucket\naws s3 cp *safetensors s3://&lt;bucket_path&gt;/&lt;model_path&gt;\n</code></pre></p>"},{"location":"user-guide/training-guide/#prepare-dataset","title":"Prepare Dataset","text":"<p>The dataset is an indispensable input for model training and fine-tuning.</p> <p>Taking the example of training a LoRa model containing specific image style, users need to prepare a set of images in advance. These images should have a consistent theme or style, with moderate resolution, and a few dozen images are sufficient. For this image set, users need to preprocess it to adapt to the capabilities of the base model. For example, it is recommended to crop the images to a size of 512 x 512 pixels in preparation for training tasks on the LoRa model based on the base model SD 1.5.</p> <p>After preprocessing, it is necessary to annotate the images in  dataset, that is, add textual descriptions to each training image and save them as text files with the same name as the corresponding images. Image annotation can be complete through the built-in image annotation function in the SD WebUI or through multi-modal large models. The annotations made by the model may not be perfect, so manual review and adjustments are recommended to ensure the final effect.</p> <p>Please refer Dataset Management in Cloud Asset Management to upload dataset to cloud.</p> <p>In addition, user could also upload dataset by execute AWS CLI command to copy the dataset to S3 bucket <pre><code>aws s3 sync local_folder_name s3://&lt;bucket_name&gt;/&lt;folder_name&gt;\n</code></pre></p> <p>Notice: The folder name should be started with a number and underline, eg. 100_demo. Each image should be paired with a txt file with the same name, eg. demo1.png, demo1.txt, the demo1.txt contains the captions of demo1.png.</p>"},{"location":"user-guide/training-guide/#train-the-lora-model","title":"Train the LoRa Model","text":"<p>After the base model and dataset have been uploaded successfully, please follow these steps: 1. Navigate to Train Management tab, select desired training instance type in Training Instance Type, select the base model type for this training job in the FM Type field (i.e., based on Stable Diffusion 1.5 or Stable Diffusion XL). Then, choose the base model for this training session using the Model option. Finally, select the dataset that this training session will depend on using the Dataset option. 2. Update the training parameters in config_params, and click Format config Params to check and correct updated params file format. 3. Click Create Training Job to submit training job. 4. Refresh Trainings List to follow status of training job. 5. The successfully trained LoRa model can be selected directly in txt2img or img2img, and used in image generation. More details refer to txt2img guide or img2img guide.</p>"},{"location":"user-guide/training-guide/#train-loss-visualization","title":"Train Loss Visualization","text":"<p>On the Train Management tab, all the finished training job will be listed in the Training List, click the training id and the logs will show the below referred to the following figure.   Click the log event link and a new web page with loss tensorboard will be appeared as the below figure. </p>"},{"location":"user-guide/training-guide/#invoke-training-api","title":"Invoke Training API","text":"<p>Refer to API document to invoke training API.</p>"},{"location":"user-guide/ComfyUI/inference/","title":"Image or Video generation through ComfyUI in the cloud","text":"<p>After successfully deploying the solution, you can open the native ComfyUI page provided by the deployed stack. The summary steps for workflow debugging, releasing, and inference are as follows:</p> <ul> <li>Step 1: Connect to the EC2 that deploys ComfyUI frontend.</li> <li>Step 2: Open the Designer link provided by the stack of solution, debug the new workflow locally (on the EC2 virtual machine), install the missing nodes, upload the required inference models, and ensure they can be successfully loaded and inferred locally (on the EC2 virtual machine).</li> <li>Step 3: Release the workflow as a template. </li> <li>Step 4: Create the endpoints needed for the workflow through API.</li> <li>Step 5: On the ComfyUI inference page, select the released template, modify the inference parameters (for example: prompts) and models as needed, and perform inference on images/videos. This will utilize Amazon SageMaker resource. </li> </ul>"},{"location":"user-guide/ComfyUI/inference/#step-1-connect-to-the-ec2-that-deploys-comfyui-frontend","title":"Step 1: Connect to the EC2 that deploys ComfyUI frontend","text":"<p>By connecting to the EC2 instance, you can check the corresponding directory structure locally, making it easier to view the log files during local workflow debugging and aid in diagnostics.</p> <p>If you only need to view the local ComfyUI debugging logs, you can also achieve this through the following steps:</p> <ol> <li>Open the EC2 console in the same region where the deployment solution is located, select the comfy-on-aws-dev instance, and click Connect in the top right corner.</li> <li>In the available connection methods, select the EC2 Instance Connect tab and click Connect.</li> <li>After a short wait, a new EC2 connection page will pop up. You can enter the required commands as needed to perform various operations. Common commands include:</li> </ol> <pre><code>tail -f /var/log/cloud-init-output.log     Used for real-time viewing of the initial logs during the EC2 startup process for Comfy.\nsudo journalctl -u comfy -f       Used for real-time viewing of Comfy runtime logs.\ntail -f /root/stable-diffusion-aws-extension/container/*.log       Used to view all logs of the Comfy runtime container.\nsudo journalctl -u comfy --no-pager -n 200        Used to view the last 200 logs of the Comfy runtime.\ndocker images -q | xargs docker rmi -f\n</code></pre>"},{"location":"user-guide/ComfyUI/inference/#step-2-debug-the-workflow","title":"Step 2: Debug the workflow","text":"<p>In the native Designer page provided by this solution, you can debug new workflows using the same methods as the local version of ComfyUI. Model management, and other tasks can be performed by connecting to the virtual machine (EC2) where ComfyUI is deployed.</p> <p></p> <p>The steps for using the senior design version of ComfyUI are summarized as follows:</p> <ol> <li>(Optional) Drag an existing workflow JSON file into the ComfyUI interface to render the workflow.</li> <li>Adjust (including adding and deleting) work nodes (custom nodes), and adjust inference parameters and the models used.</li> <li>Click Queue Prompt to start an inference task based on the current page workflow.</li> <li> <p>(Optional) If step 3 results in an error prompt, follow the instructions to resolve it. For example, if a missing model is indicated, download the model to the corresponding directory on the EC2 instance; if missing custom nodes are indicated, click Manager and then Install Missing Custom Nodes to install the missing nodes. After resolving the error, repeat step 3 to test again. </p> </li> <li> <p>When the workflow is completed and the generated results are displayed on the interface, it indicates that the workflow debugging has been successful.</p> </li> </ol>"},{"location":"user-guide/ComfyUI/inference/#step-3-release-workflow-as-new-template","title":"Step 3: Release workflow as new template","text":"<p>Once the workflow can successfully infer images/videos locally (on the EC2 virtual machine), you can follow these steps to quickly release the debugged workflow as a template, making it convenient and stable for future inference calls through Amazon SageMaker.</p> <ol> <li>Click New Workflow in the right navigation bar or the plus sign above the workflow list module on the right side.</li> <li> <p>In the pop-up window, enter the name of the template to be published and click OK.</p> <p>Tip</p> <p>The new template name must not exceed 20 characters in length, combining letters and numbers. It is case-sensitive. Additionally, the name must be unique within the same region. If it conflicts with an existing template name, an error message will be displayed.</p> </li> <li> <p>During the workflow publishing process, no updates should be made on the ComfyUI frontend. Once the publishing is complete, a pop-up notification will confirm the successful publication.</p> </li> <li>During publication, the workflow you are currently debugging will be temporarily saved. Each time you switch environments, the temporarily saved workflow corresponding to the current environment will be loaded first.</li> </ol>"},{"location":"user-guide/ComfyUI/inference/#step-4-deploy-new-inference-endpoint-for-future-inference-of-released-workflow","title":"Step 4: Deploy new inference endpoint for future inference of released workflow","text":"<p>After completing the workflow releasing, you'll need to create a Amazon SageMaker inference endpoint to perform cloud-based inference based on the workflow:</p> <ol> <li> <p>You'll need to call the creation via API, referencing the \"Deploying New Amazon SageMaker Inference Nodes\" subsection in this documentation.</p> </li> <li> <p>Once the inference endpoint is created and in the InService state, the published workflow will be ready for inference.</p> </li> </ol>"},{"location":"user-guide/ComfyUI/inference/#step-5-inference-of-released-template","title":"Step 5: Inference of released template","text":"<p>In the view of Designer or InferencePot, you can easily perform inference based on a template using the following steps:</p> <ol> <li> <p>Open the ComfyUI page and select a released template from the right-hand navigation bar. If in Designer view, also need to select the Prompt on AWS checkbox in the right-hand navigation bar.</p> </li> <li> <p>The selected template will be automatically render in the ComfyUI page. Adjust the parameters as needed, and click Queue Prompt to submit the inference task.</p> </li> <li> <p>Once the inference task is completed, the generated results will automatically be displayed on the page.</p> </li> </ol>"},{"location":"user-guide/ComfyUI/inference/#manage-models","title":"Manage model(s)","text":""},{"location":"user-guide/ComfyUI/inference/#upload-model","title":"Upload model","text":"<p>To ensure smooth access during the model debugging phase, new models need to be uploaded to EC2. You can achieve this by entering the corresponding subfolder for the model category under the models directory on EC2. Use either direct drag-and-drop or the wget command with the model download URL. Considering network speed, it is recommended to prioritize using the wget method for model downloads.</p>"},{"location":"user-guide/webUI/CloudAssetsManage/","title":"Main Tab","text":"<p>This chapter will provide a detailed overview of the convenient cloud-based resource management approach offered by this solution.</p>"},{"location":"user-guide/webUI/CloudAssetsManage/#upload-model","title":"Upload Model","text":"<p>To use extra models for inference, you could upload model through steps below in two ways, and follow steps in txt2img or img2imgto inference with extra models as need.</p> <p>Method One: 1. Within Stable Diffusion WebUI, navigate to solution main tab Amazon SageMaker, find session Cloud Models Management.  2. Select the from WebUI tab, which means to upload the model from the models file path where the WebUI service is deployed. 3. Enter the model path where the WebUI service is deployed under corresponding model text box.</p> <p>Note: You can upload multiple kinds of models by entering multiple local model paths in text box. 4. Click Upload Models to Cloud to start uploading process. 5.Message will appear on left right once uploading completes.</p> <p>Method Two: 1. Within Stable Diffusion WebUI, navigate to solution main tab Amazon SageMaker main tab, find session Cloud Models Management.  2. Select the from Laptop tab, which means to upload the model from the local path to access the WebUI. 3. Select the type of model to upload, currently supports six types: SD Checkpoints, Textual Inversion, LoRA model, ControlNet model, Hypernetwork, VAE 3. Select the model file to be uploaded locally.</p> <p>Note: You can select models multiple, but subject to browser restrictions, it is best to select no more than 10 files, and the total size should not exceed 8g. 4. Click Upload Models to Cloud to start uploading process. 5. The upload will be uploaded in pieces asynchronously based on the file size and quantity. After each piece is uploaded, you will see a prompt under the Choose File button</p> <p>Method Three: 1. Within Stable Diffusion WebUI, navigate to the Amazon SageMaker main tab and find session Cloud Models Management. 2. Select the from URL tab. This option allows you to upload models to S3 from URLs where the models are downloaded.  3. Choose the type of model you want to upload. Currently, six types are supported: SD Checkpoints, Textual Inversion, LoRA model, ControlNet model, Hypernetwork, VAE. 4. In the URL list (Comma-separated in English) input box, enter the URL list of model downloads, separated by commas. 5. In the Models Description (Optional) input box, provide a JSON-formatted description (optional).</p> <p>Note: You can select multiple model files, but it's advisable not to exceed 5 files in your selection, with a total size not exceeding 12 GB, as constrained by Lambda memory and concurrent thread limits. 6. Click Upload Models to Cloud to start the model upload process. 7. You will see a prompt in the Label section below indicating the progress of the upload request.</p>"},{"location":"user-guide/webUI/CloudAssetsManage/#amazon-sagemaker-endpoint-management","title":"Amazon SageMaker Endpoint Management","text":""},{"location":"user-guide/webUI/CloudAssetsManage/#deploy-new-endpoint","title":"Deploy new endpoint","text":"<ol> <li>Navigate to the Amazon SageMaker main tab of the solution. In the Cloud Assets Management module, locate the Deploy New SageMaker Endpoint section.</li> <li>The default deployment type for the solution is ml.g5.2xlarge, with 1 instance. The endpoint autoscaling feature is enabled by default. Simply click the Deploy button to initiate the deployment of the Sagemaker endpoint.</li> <li> <p>If users wish to specify the endpoint name, instance type, and maximum instance count for the endpoint's instances, they can check the Advanced Endpoint Configuration checkbox. This will display additional parameters for user input. The following table lists the names and descriptions of these parameters:</p> Parameter Name Description Endpoint Name (Optional) If you need to specify a name for the Sagemaker endpoint, enter it in this input box. If not modified, the default endpoint name will be esd-type-XXXXX. Endpoint Type Select the inference type Async/Real time for the deployed Endpoint Instance Type Select the instance type for the deployed endpoint from the dropdown list. Max Instance Number Choose the maximum number of instances for the deployed endpoint from the dropdown list. If Autoscaling is selected, Sagemaker will elastically scale between 0 and the Max Instance Number based on average CPU usage. Enable Autoscaling If this checkbox is selected, Async inference will scale elastically between 0 and Max Instance Numbers based on the average backlog of each instance, while Real-time inference will scale elastically between 1 and Max Instance Numbers based on the average number of calls per instance. Min Instance Number If Enable Autoscaling is true, This value will be the minimum number of Endpoint instances </li> <li> <p>After selecting the default endpoint configuration or setting up the advanced endpoint configuration, click Deploy. You'll see a message indicating Endpoint deployment started under Label.    </p> </li> <li> <p>You can navigate to tab Amazon SageMaker, session Inference Endpoint Management, refresh list to check all the deployment status of endpoints.</p> <p>Note: The format of the drop down list is\uff1aendpoint name+ deployment status (including Creating/Failed/InService)+deployment completing time.</p> </li> <li> <p>It will take around 3 minutes for endpoint deployment status changing to InService, which indicates that the endpoint has been successfully deployed.</p> </li> </ol>"},{"location":"user-guide/webUI/CloudAssetsManage/#delete-deployed-endpoints","title":"Delete deployed endpoints","text":"<ol> <li>Refresh and select endpoint(s) under the dropdown list of Inference Endpoint Management.</li> <li>Click Delete, message Endpoint delete completed will appear on the left side, which indicates that the selected endpoint(s) has been successfully deleted.</li> </ol>"},{"location":"user-guide/webUI/CloudAssetsManage/#aws-dataset-management","title":"AWS Dataset Management","text":""},{"location":"user-guide/webUI/CloudAssetsManage/#create-dataset","title":"Create Dataset","text":"<p>In functions such as model fine-tuning, it is necessary to provide a file of images for fine-tuning work. This functional module helps users quickly upload images to the cloud.</p> <ol> <li> <p>Navigate to main tab Amazon SageMaker, section AWS Dataset Management\uff0csub-tab Create. </p> </li> <li> <p>Click Click to Upload a File, in the local file browser that pops up, confirm to select all the images required for one model fine-tuning.</p> </li> <li>Enter file name in Dataset Name, enter file description in Dataset Description, click Create Dataset.</li> <li>The message Complete Dataset XXXX creation will be displayed on the right side once the process completes.</li> </ol>"},{"location":"user-guide/webUI/CloudAssetsManage/#explore-dataset","title":"Explore Dataset","text":"<p>After the dataset has been successfully uploaded, this feature module allows users to quickly obtain the corresponding cloud-based address of the dataset. Users can copy this address and paste it into the location where the collection of images needs to be uploaded.</p> <ol> <li>Navigate to Amazon SageMaker\uff0cAWS Dataset Management session\uff0cBrowse tab, refresh the list Dataset From Cloud and select desired dataset.</li> <li>Field dataset s3 location will display the corresponding S3 path on the cloud. User can copy to use as need.</li> </ol>"},{"location":"user-guide/webUI/controlnet-guide/","title":"Use Controlnet for inference","text":"<p>You can open controlnet sub-session, by combining the use of native functionalities txt2img or img2img along with the added panel of Amazon SageMaker Inference in the solution, the inference tasks involving cloud resources can be invoked.</p>"},{"location":"user-guide/webUI/controlnet-guide/#controlnet-user-guide","title":"Controlnet User Guide","text":""},{"location":"user-guide/webUI/controlnet-guide/#multi-controlnet-user-guide","title":"Multi-controlnet user guide","text":"<ol> <li> <p>Navigate to Settings tab. In the left sidebar, select ControlNet, and adjust the Multi ControlNet in the right panel: Max models amount (requires restart) setting to specify the number of ControlNets (1-10). Afterward, restart the webUI for the changes to take effect, and the Multi ControlNet configuration will be active. </p> </li> <li> <p>Navigate to txt2img or img2img tab\uff0can equal number of ControlNet units will appear. For example, below shows 3 controlNet units started simultaneously. </p> </li> </ol>"},{"location":"user-guide/webUI/controlnet-guide/#openpose-user-guide","title":"openpose User Guide","text":"<ol> <li>Open ControlNet panel, choose ControlNet Unit 0, check Enable, select openpose from Preprocessor, and then upload am image. </li> <li>Similar to local inference, you can customize the inference parameters of the native ControlNet. The controlnet model \"control_openpose-fp16.safetensors\" should be uploaded to the cloud before generate. </li> <li>Click Generate on Cloud after finished all parameters setting.</li> <li>Refresh and select the top Inference Job from Inference Job: Time-Type-Status-Uid, inference result will be displayed in Output section.</li> <li>Subsequent actions. You can click Save or others as need to perform further processing. </li> </ol>"},{"location":"user-guide/webUI/endpoint-autoscaling/","title":"SageMaker Async Endpoint Autoscaling","text":"<p>Amazon SageMaker provides capabilities to automatically scale model inference endpoints in response to the changes in traffic patterns. This document explains how autoscaling is enabled for an Amazon SageMaker async endpoint created by this Solution</p>"},{"location":"user-guide/webUI/endpoint-autoscaling/#overview","title":"Overview","text":"<p>The solution provided enables autoscaling for a specific endpoint and variant in Amazon SageMaker. Autoscaling is managed through two scaling policies:</p> <ol> <li>Target Tracking Scaling Policy: This policy adjusts the desired instance count based on the <code>CPUUtilization</code> metric. It aims to keep the CPU utilization at 50%. If the average CPU utilization is above 50 for 5 minutes, the alarm will trigger application autoscaling to scale out Sagemaker endpoint until it reach the maximum number of instances.</li> </ol> <p>The scaling policy based on CPU utilization is defined using the <code>put_scaling_policy</code> method. It specifies the following parameters:     - <code>TargetValue</code>: 50% CPU utilization     - <code>ScaleInCooldown</code>: 300 seconds     - <code>ScaleOutCooldown</code>: 300 seconds</p> <ol> <li>Step Scaling Policy: This policy allows you to define steps for scaling adjustments based on the <code>HasBacklogWithoutCapacity</code> metric. This policy is created to let application autoscaling increase the instance number from 0 to 1 when there is inference request but endpoint has 0 instance.</li> </ol> <p>The step scaling policy is defined to adjust the capacity based on the <code>HasBacklogWithoutCapacity</code> metric. It includes: - <code>AdjustmentType</code>: ChangeInCapacity - <code>MetricAggregationType</code>: Average - <code>Cooldown</code>: 300 seconds - <code>StepAdjustments</code>: Specifies the scaling adjustments based on the size of the alarm breach.</p>"},{"location":"user-guide/webUI/endpoint-autoscaling/#example-of-sagemaker-async-endpoint-autoscaling-policy-below","title":"Example of Sagemaker async endpoint autoscaling policy below:","text":"<pre><code>{\n    \"ScalingPolicies\": [\n        {\n            \"PolicyARN\": \"Your PolicyARN\",\n            \"PolicyName\": \"HasBacklogWithoutCapacity-ScalingPolicy\",\n            \"ServiceNamespace\": \"sagemaker\",\n            \"ResourceId\": \"endpoint/esd-type-c356f91/variant/prod\",\n            \"ScalableDimension\": \"sagemaker:variant:DesiredInstanceCount\",\n            \"PolicyType\": \"StepScaling\",\n            \"StepScalingPolicyConfiguration\": {\n                \"AdjustmentType\": \"ChangeInCapacity\",\n                \"StepAdjustments\": [\n                    {\n                        \"MetricIntervalLowerBound\": 0.0,\n                        \"ScalingAdjustment\": 1\n                    }\n                ],\n                \"Cooldown\": 300,\n                \"MetricAggregationType\": \"Average\"\n            },\n            \"Alarms\": [\n                {\n                    \"AlarmName\": \"stable-diffusion-hasbacklogwithoutcapacity-alarm\",\n                    \"AlarmARN\": \"Your AlarmARN\"\n                }\n            ],\n            \"CreationTime\": \"2023-08-14T13:53:10.480000+08:00\"\n        },\n        {\n            \"PolicyARN\": \"Your PolicyARN\",\n            \"PolicyName\": \"CPUUtil-ScalingPolicy\",\n            \"ServiceNamespace\": \"sagemaker\",\n            \"ResourceId\": \"endpoint/esd-type-c356f91/variant/prod\",\n            \"ScalableDimension\": \"sagemaker:variant:DesiredInstanceCount\",\n            \"PolicyType\": \"TargetTrackingScaling\",\n            \"TargetTrackingScalingPolicyConfiguration\": {\n                \"TargetValue\": 50.0,\n                \"CustomizedMetricSpecification\": {\n                    \"MetricName\": \"CPUUtilization\",\n                    \"Namespace\": \"/aws/sagemaker/Endpoints\",\n                    \"Dimensions\": [\n                        {\n                            \"Name\": \"EndpointName\",\n                            \"Value\": \"esd-type-c356f91\"\n                        },\n                        {\n                            \"Name\": \"VariantName\",\n                            \"Value\": \"prod\"\n                        }\n                    ],\n                    \"Statistic\": \"Average\",\n                    \"Unit\": \"Percent\"\n                },\n                \"ScaleOutCooldown\": 300,\n                \"ScaleInCooldown\": 300\n            },\n            \"Alarms\": [\n                {\n                    \"AlarmName\": \"TargetTracking-endpoint/esd-type-c356f91/variant/prod-AlarmHigh-c915b303-9048-40b2-99a7-f5b7e49ab7c4\",\n                    \"AlarmARN\": \"Your AlarmARN\"\n                },\n                {\n                    \"AlarmName\": \"TargetTracking-endpoint/esd-type-c356f91/variant/prod-AlarmLow-2fd61f99-c2e5-4ac6-9722-54030c3f0216\",\n                    \"AlarmARN\": \"Your AlarmARN\"\n                }\n            ],\n            \"CreationTime\": \"2023-08-14T13:53:10.182000+08:00\"\n        }\n    ]\n}\n</code></pre>"},{"location":"user-guide/webUI/extensions-guide/","title":"Other Extensions User Guide","text":""},{"location":"user-guide/webUI/extensions-guide/#extension-reactor-for-faceswap","title":"Extension ReActor for FaceSwap","text":"<p>You can open the ReActor tab session, combining native windows of txt2img or img2img with solution tab Amazon SageMaker Inference, to achieve faceswap feature on cloud.</p>"},{"location":"user-guide/webUI/extensions-guide/#user-guide","title":"User Guide","text":"<p>Here is an example of using ReActor in txt2img to introduce the recommended steps. 1. Select a base model under Stable Diffusion Checkpoint Used on Cloud, the Generate button will automatically change to \"generate on cloud\". 2. Assuming you want to generate an image of a girl with the appearance of the Mona Lisa, fill in the prompt box with \"a girl\". 3. Open tab of ReActor, drag image of Mona Lisa into Single Source Image.  4. Click Generate on cloud, and result will be presented in Output session as below. </p> <p>Reactor supports multi-face swapping simultaneously by specifying the index of the face. It also allows loading face models for swapping. For more detailed usage instructions, please refer to the extension documentation</p>"},{"location":"user-guide/webUI/extensions-guide/#extension-tiled-diffusion-tiled-vae-for-image-super-resolution","title":"Extension Tiled Diffusion &amp; Tiled VAE for Image Super Resolution","text":"<p>You can use tab of Tiled VAE, combining with native txt2img or img2img with solution tab Amazon SageMaker Inference, in order to achieve image super resolution work. </p>"},{"location":"user-guide/webUI/extensions-guide/#user-guide_1","title":"User Guide","text":"<p>By utilizing these two extensions, it is able to generate ultra-high-resolution images within limited VRAM. Here, we'll take the example of using these extensions in txt2img to demonstrate how to perform super-resolution inference:</p> <ol> <li>Select a base model under Stable Diffusion Checkpoint Used on Cloud, the Generate button will automatically change to \"generate on cloud\".</li> <li>Assuming you want to generate an image of a cat, fill in the prompt box with \"a cat\".</li> <li>Open Tiled Diffusion Hires.fix, and set the upscaling factor to 4x for super-resolution. </li> <li>Open Tiled VAE\uff0cEnable Tiled VAE, you can generate the image using the default parameters. </li> <li>Click Generate on cloud, and result will be presented in Output session. </li> </ol> <p>Tiled Diffusion supports setting different prompts for different regions to generate ultra-high-resolution images. For detailed usage instructions, please refer to the extension documentation</p>"},{"location":"user-guide/webUI/img2img-guide/","title":"img2img Guide","text":"<p>You can open the img2img tab and use the original region along with the Amazon SageMaker Inference to perform inference on the cloud.</p>"},{"location":"user-guide/webUI/img2img-guide/#img2img-user-guide","title":"img2img user guide","text":""},{"location":"user-guide/webUI/img2img-guide/#standard-process-for-different-functional-labels-in-img2img","title":"Standard process for different functional labels in img2img","text":"<ol> <li>Navigate to tab img2img, find panel Amazon SageMaker Inference.</li> <li> <p>Input parameters for inference. The same as local inference, you could edit parameters in native fields for model name (stable diffusion checkpoint, extra networks:Lora, Hypernetworks, Textural Inversion and VAE), prompts, negative prompts, sampling parameters, inference parameters and etc. For functions img2img, sketch, inpaint, inpaint sketch and inpaint upload, you could upload and modify images in the native way.</p> <p>Notice</p> <p>The model files used in the inference should be uploaded to the cloud before generate, which can be referred to the introduction of chapter Cloud Assets Management</p> <ol> <li>Select Stable Diffusion checkpoint model that will be used for cloud inference through Stable Diffusion Checkpoint Used on Cloud, and the button Generate will change to button Generate on Cloud. </li> </ol> <p>Notice</p> <p>This field is mandatory. If you choose an endpoint that is in any other state or leave it empty, an error will occur when you click Generate on Cloud to initiate cloud-based inference.</p> </li> <li> <p>ClickGenerate on Cloud\u3002</p> </li> <li>Check inference result. Fresh and select the top option among Inference Job Histories: Time-Type-Status-UUID dropdown list. The Output section in the top-right area of the img2img tab will display the results of the inference once completed, including the generated images, prompts, and inference parameters. Based on this, you can perform subsequent workflows such as clicking Save or Send to extras. <p>Note: The list is sorted in reverse chronological order based on the inference time, with the most recent inference task appearing at the top. Each record is named in the format of inference time -&gt; job type(txt2img/img2img/interrogate_clip/interrogate_deepbooru) -&gt; inference status(succeed/in progress/fail) -&gt;inference id.</p> </li> </ol>"},{"location":"user-guide/webUI/img2img-guide/#img2img-label","title":"img2img label","text":"<ol> <li>Upload the original image to img2img and enter prompts, and click Generate on Cloud.</li> <li>Select corresponding Inference Job ID, the generated image will present on the right Output session. </li> </ol>"},{"location":"user-guide/webUI/img2img-guide/#sketch-label","title":"Sketch label","text":"<ol> <li>Start Stable Diffusion WebUI with \u2018--gradio-img2img-tool color-sketch\u2019 on the command line\uff0cupload the whiteboard background image to the Sketch tab.</li> <li> <p>Use a brush to draw the corresponding sketch and prepare prompts, and click Generate on Cloud. </p> </li> <li> <p>Select corresponding Inference Job ID, the generated image will present on the right Output session. </p> </li> </ol>"},{"location":"user-guide/webUI/img2img-guide/#inpaint-label","title":"Inpaint label","text":"<ol> <li>Upload original image to Inpaint label.</li> <li> <p>Establish masks with brushes and prepare prompts, and click Generate on Cloud. </p> </li> <li> <p>Select corresponding Inference Job ID, the generated image will present on the right Output session. </p> </li> </ol>"},{"location":"user-guide/webUI/img2img-guide/#inpaint-sketch-label","title":"Inpaint Sketch label","text":"<ol> <li>Start Stable Diffusion WebUI with \u2018--gradio-img2img-tool color-sketch\u2019\uff0cand upload original image into Inpaint Sketchlabel with prompts.</li> <li> <p>Establish masks with brushes, and click Generate on Cloud. </p> </li> <li> <p>Select corresponding Inference Job ID, the generated image will present on the right Output session. </p> </li> </ol>"},{"location":"user-guide/webUI/img2img-guide/#inpaint-upload-label","title":"Inpaint Upload label","text":"<ol> <li> <p>Upload original image and mask image to Inpaint Upload label and prepare prompts, for example large eyes. </p> </li> <li> <p>Click Generate on Cloud, and select corresponding Inference Job ID, the generated image will present on the right Output session. </p> </li> </ol>"},{"location":"user-guide/webUI/img2img-guide/#interrogate-clipdeepbooru","title":"Interrogate clip/deepbooru","text":"<ol> <li>Navigate to img2img tab\uff0copen Amazon SageMaker Inference panel.</li> <li> <p>Interrogate only need to upload image to img2img tab. </p> </li> <li> <p>Click Interrogate CLIP on cloud or Interrogate DeepBooru on cloud.</p> </li> <li>Check inference result. Refresh the dropdown list of Inference Job JDs, check the topmost Inference Job ID that match the inference submission timestamp to review the inference result. </li> </ol>"},{"location":"user-guide/webUI/img2img-guide/#continuous-inference-scenario","title":"Continuous Inference Scenario","text":"<ol> <li>Following the General Inference Scenario, complete the parameter inputs and click Generate on Cloud to submit the initial inference task.</li> <li>Wait for the appearance of a new Inference ID in the right-side Output section.</li> <li>Once the new Inference ID appears, you can proceed to click Generate on Cloud again for the next inference task.</li> </ol>"},{"location":"user-guide/webUI/multi-user/","title":"Configure API and Multiple Users.","text":""},{"location":"user-guide/webUI/multi-user/#configure-api","title":"Configure API","text":"<ol> <li>Open AWS CloudFormation Console, and select the main stack that successfully deployed.</li> <li>Navigate the Outputs tab, and copy the information under APIGatewayUrl and ApiGatewayUrlToken.</li> <li>Open Stable Diffusion webUI, navigate to the 'Amazon SageMaker' tab, paste information from step 2 into fields API URL and API Token. Create a super admin user name and password by entering information in field Username and Password. And click Test Connection &amp; Update Setting.</li> <li>Message Successfully Connected &amp; Setting Updated will be printed once front UI successfully connected with backend cloud resource. The configuration file has been updated to automatically display corresponding information upon future launches of the webUI.</li> </ol>"},{"location":"user-guide/webUI/multi-user/#multiple-user-management","title":"Multiple User Management","text":"<p>IT Operator role users can log in and navigate to the Amazon SageMaker tab, and then to the API and User Settings sub-tab to manage roles and users.</p>"},{"location":"user-guide/webUI/multi-user/#role-management","title":"Role Management","text":"<p>Under the Role Management tab, roles can be viewed, created, and configured with corresponding permissions. After creating a new role, click Next Page or refresh the page to update the Role Table.</p>"},{"location":"user-guide/webUI/multi-user/#permissions-description","title":"Permissions Description","text":"Name Scope Permissions role:all Role Create role, obtain role list, delete role user:all User Create users, obtain user lists, delete users, update users sagemaker_endpoint:all Endpoint Create endpoints, obtain endpoint list, delete endpoints inference:all Inference Create and start reasoning tasks, obtain reasoning task list, delete reasoning tasks checkpoint:all Checkpoint Create model files, obtain a list of model files, delete model files train:all Training Create training assignments, obtain training assignment list, delete training assignments"},{"location":"user-guide/webUI/multi-user/#users-management","title":"Users Management","text":""},{"location":"user-guide/webUI/multi-user/#add-new-user","title":"Add New User","text":"<ol> <li> <p>To meet your specific requirements, create new users, passwords, and roles. Once you click on \"Next Page,\" the newly created users will be visible. To ensure the configuration changes related to the new users take effect in the web UI server, it is necessary to restart the web UI again. </p> </li> <li> <p>Open another incognito browser, and log in using the newly created username and password.</p> </li> <li>When accessing the Amazon SageMaker tab, the displayed content may vary for different users.</li> </ol>"},{"location":"user-guide/webUI/multi-user/#manage-existing-user","title":"Manage Existing User","text":"<ol> <li>Select the corresponding user from User Table that is expected to be updated, including Password or User Role update. The user information will be displayed in field Update a User Setting.</li> <li>Update the corresponding fields as need, and click Upsert a User to save the change. Otherwise click Delete a User to delete the selected user.</li> </ol>"},{"location":"user-guide/webUI/txt2img-guide/","title":"txt2img Guide","text":"<p>You can open the txt2img tab to perform text-to-image inference using the combined functionality of the native region of txt2img and the newly added \"Amazon SageMaker Inference\" panel in the solution. This allows you to invoke cloud resources for txt2img inference tasks.</p>"},{"location":"user-guide/webUI/txt2img-guide/#txt2img-user-guide","title":"txt2img user guide","text":""},{"location":"user-guide/webUI/txt2img-guide/#general-inference-scenario","title":"General Inference Scenario","text":"<ol> <li>Navigate to txt2img tab, find Amazon SageMaker Inference panel.  </li> <li> <p>Enter the required parameters for inference. Similar to local inference, you can customize the inference parameters of the native txt2img, including model name (stable diffusion checkpoint, extra networks:Lora, Hypernetworks, Textural Inversion and VAE), prompts, negative prompts, sampling parameters, and inference parameters. For VAE model switch, navigate to Settings tab, select Stable Diffusion in the left panel, and then select VAE model in SD VAE (choose VAE model: Automatic = use one with same filename as checkpoint; None = use VAE from checkpoint). </p> <p>Notice</p> <p>The model files used in the inference should be uploaded to the cloud before generate, which can be referred to the introduction of chapter Cloud Assets Management. The current model list displays options for both local and cloud-based models. For cloud-based inference, it is recommended to select models with the sagemaker keyword as a suffix, indicating that they have been uploaded to the cloud for subsequent inference.</p> </li> <li> <p>Select Stable Diffusion checkpoint model that will be used for cloud inference through Stable Diffusion Checkpoint Used on Cloud, and the button Generate will change to button Generate on Cloud. </p> <p>Notice</p> <p>This field is mandatory. </p> </li> <li> <p>Finish setting all the parameters, and then click Generate on Cloud.</p> </li> <li> <p>Check inference result. Fresh and select the top option among Inference Job dropdown list, the Output section in the top-right area of the txt2img tab will display the results of the inference once completed, including the generated images, prompts, and inference parameters. Based on this, you can perform subsequent workflows such as clicking Save or Send to img2img.</p> <p>Note\uff1a The list is sorted in reverse chronological order based on the inference time, with the most recent inference task appearing at the top. Each record is named in the format of inference time -&gt; inference id.</p> </li> </ol> <p></p>"},{"location":"user-guide/webUI/txt2img-guide/#inference-using-extra-models-like-lora","title":"Inference Using Extra Model(s) like Lora","text":"<ol> <li>Please follow the native version of WebUI, and upload a copy of the required model (including Textual Inversion, Hypernetworks, Checkpoints or Lora models) to local machine.</li> <li>Upload corresponding models to the cloud, following Upload Models.</li> <li>Select the required model, adjust the weights of the model in prompts field, and click Generate on Cloud to inference images.</li> </ol>"},{"location":"user-guide/webUI/txt2img-guide/#inference-job-histories","title":"Inference Job Histories","text":"<p>Inference Job displays the latest 10 inference jobs by default, following naming format Time-Type-Status-UUID. Checking Show All will display all inference jobs the account has. Checking Advanced Inference Job filter and apply filters as need will provide users a customized inference job list.</p> <p>The inference API combinations and corresponding parameters for a specific historical inference task can be obtained in API Debugger.</p>"}]}